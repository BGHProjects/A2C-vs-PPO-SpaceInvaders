{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Advantage Actor Critic (A2C) and Proximal Policy Optimization (PPO) Reinforcement Learning Algorithms using Space Invaders\n",
    "- Each algorithm will be trained with default parameters and 1000 steps initially then 20000 steps after\n",
    "- The algorithms will be playing the Atari Space Invaders game, aiming to achieve the best result\n",
    "Libraries/Software used:\n",
    "- Python OpenAI Gym (https://gym.openai.com/)\n",
    "- Stable Baselines 3 (https://stable-baselines3.readthedocs.io/en/master/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra code required to load all of the Atari game emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to load the atari environments and get the ROMS to work\n",
    "# References local files (see file structure)\n",
    "# http://www.atarimania.com/roms/Roms.rar\n",
    "# https://stackoverflow.com/questions/67656740/exception-rom-is-missing-for-ms-pacman-see-https-github-com-openai-atari-py\n",
    "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
    "!pip install unrar\n",
    "!unrar x Roms.rar\n",
    "!mkdir rars\n",
    "!mv HC\\ ROMS.zip   rars\n",
    "!mv ROMS.zip  rars\n",
    "!python -m atari_py.import_roms rars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "Box([[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]], [[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]], (210, 160, 3), uint8)\n",
      "Episode:1 Score:210.0 \n",
      "Episode:2 Score:105.0 \n",
      "Episode:3 Score:210.0 \n",
      "Episode:4 Score:120.0 \n",
      "Episode:5 Score:185.0 \n"
     ]
    }
   ],
   "source": [
    "environment_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(environment_name)\n",
    "\n",
    "# View the Action Space and Observation Space, to see what is available (and also data types)\n",
    "env.reset()\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "# Testing and visualising the environment\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        # Renders the environment visually\n",
    "        env.render()\n",
    "        # Takes a random action from the action space\n",
    "        action = env.action_space.sample()\n",
    "        # The four values returned from the observation\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{} '.format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorise the Environment\n",
    "This is so we can train multiple models at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains four environments at the same time\n",
    "env = make_atari_env('SpaceInvaders-v0', n_envs=4, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Evaluate, and Test the different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 - A2C (Advantage Actor Critic Method)\n",
    "- Uses multiple workers to avoid the use of a replay buffer\n",
    "- https://stable-baselines.readthedocs.io/en/master/modules/a2c.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to Training\\Logs\\A2C_8\n"
     ]
    }
   ],
   "source": [
    "# Sets the path to save the training logs to\n",
    "log_path = os.path.join('Training','Logs')\n",
    "\n",
    "# CNN is used, because this environment uses image-based observations\n",
    "a2c_1000_model = A2C('CnnPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "# 1000 steps initially just to test\n",
    "a2c_1000_model.learn(total_timesteps=1000)\n",
    "\n",
    "# Save the model after training\n",
    "a2c_path = os.path.join('Training','Saved Models','A2C_Breakout_Model')\n",
    "a2c_1000_model.save(a2c_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate A2C 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to just one environment in order to evaluate it\n",
    "env = make_atari_env('SpaceInvaders-v0', n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "evaluate_policy(a2c_1000_model, env, n_eval_episodes=10, render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test A2C 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[0.] \n",
      "Episode:2 Score:[8.] \n",
      "Episode:3 Score:[9.] \n",
      "Episode:4 Score:[2.] \n",
      "Episode:5 Score:[10.] \n",
      "Episode:6 Score:[9.] \n",
      "Episode:7 Score:[0.] \n",
      "Episode:8 Score:[2.] \n",
      "Episode:9 Score:[1.] \n",
      "Episode:10 Score:[0.] \n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = a2c_1000_model.predict(obs) # Now using the model\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{} '.format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, Evaluate, and Test again, but this time with 20000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to Training\\Logs\\A2C_9\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 690      |\n",
      "|    ep_rew_mean        | 124      |\n",
      "| time/                 |          |\n",
      "|    fps                | 99       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.0797   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -1.04    |\n",
      "|    value_loss         | 3.1      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 763      |\n",
      "|    ep_rew_mean        | 154      |\n",
      "| time/                 |          |\n",
      "|    fps                | 100      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.13    |\n",
      "|    explained_variance | 0.865    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.24     |\n",
      "|    value_loss         | 0.338    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 738      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 100      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 59       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0.696    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.26    |\n",
      "|    value_loss         | 0.057    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 702      |\n",
      "|    ep_rew_mean        | 130      |\n",
      "| time/                 |          |\n",
      "|    fps                | 100      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 79       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.168    |\n",
      "|    value_loss         | 0.0988   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 690      |\n",
      "|    ep_rew_mean        | 122      |\n",
      "| time/                 |          |\n",
      "|    fps                | 100      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 99       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.41    |\n",
      "|    explained_variance | 0.463    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.19    |\n",
      "|    value_loss         | 2.28     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 688      |\n",
      "|    ep_rew_mean        | 126      |\n",
      "| time/                 |          |\n",
      "|    fps                | 100      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 119      |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0.0855   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.463   |\n",
      "|    value_loss         | 2.35     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 699      |\n",
      "|    ep_rew_mean        | 134      |\n",
      "| time/                 |          |\n",
      "|    fps                | 100      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 138      |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0.709    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.0402   |\n",
      "|    value_loss         | 0.162    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 709      |\n",
      "|    ep_rew_mean        | 138      |\n",
      "| time/                 |          |\n",
      "|    fps                | 101      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 158      |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0.216    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.181    |\n",
      "|    value_loss         | 0.197    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 722      |\n",
      "|    ep_rew_mean        | 144      |\n",
      "| time/                 |          |\n",
      "|    fps                | 101      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 177      |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.167   |\n",
      "|    value_loss         | 0.0771   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 698      |\n",
      "|    ep_rew_mean        | 142      |\n",
      "| time/                 |          |\n",
      "|    fps                | 101      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 197      |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.823    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.158    |\n",
      "|    value_loss         | 0.196    |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Revert to 4 environments\n",
    "env = make_atari_env('SpaceInvaders-v0', n_envs=4, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Train\n",
    "a2c_20000_model = A2C('CnnPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "a2c_20000_model.learn(total_timesteps=20000)\n",
    "a2c_path = os.path.join('Training','Saved Models','A2C_SpaceInvaders_Model')\n",
    "a2c_20000_model.save(a2c_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate (Revert back to 1 environment)\n",
    "env = make_atari_env('SpaceInvaders-v0', n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "evaluate_policy(a2c_20000_model, env, n_eval_episodes=10, render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[1.] \n",
      "Episode:2 Score:[2.] \n",
      "Episode:3 Score:[10.] \n",
      "Episode:4 Score:[7.] \n",
      "Episode:5 Score:[0.] \n",
      "Episode:6 Score:[1.] \n",
      "Episode:7 Score:[2.] \n",
      "Episode:8 Score:[3.] \n",
      "Episode:9 Score:[9.] \n",
      "Episode:10 Score:[6.] \n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = a2c_20000_model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{} '.format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion of A2C\n",
    "- Appeared to have sporadic, inconsistent results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 - PPO (Proximal Policy Optimization Algorithm)\n",
    "- Combines idea of multiple workers with a trust region to improve actor\n",
    "- New policy should not be too different from old policy (uses clipping)\n",
    "- https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initially Train, Evaluate, and Test for 1000 training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to Training\\Logs\\PPO_12\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 661      |\n",
      "|    ep_rew_mean     | 139      |\n",
      "| time/              |          |\n",
      "|    fps             | 113      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Revert to 4 environments\n",
    "env = make_atari_env('SpaceInvaders-v0', n_envs=4, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Train\n",
    "ppo_1000_model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "ppo_1000_model.learn(total_timesteps=1000)\n",
    "ppo_path = os.path.join('Training','Saved Models','PPO_SpaceInvaders_Model')\n",
    "ppo_1000_model.save(ppo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate (Revert back to 1 environment)\n",
    "env = make_atari_env('SpaceInvaders-v0', n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "evaluate_policy(ppo_1000_model, env, n_eval_episodes=10, render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[6.] \n",
      "Episode:2 Score:[1.] \n",
      "Episode:3 Score:[2.] \n",
      "Episode:4 Score:[2.] \n",
      "Episode:5 Score:[3.] \n",
      "Episode:6 Score:[4.] \n",
      "Episode:7 Score:[12.] \n",
      "Episode:8 Score:[2.] \n",
      "Episode:9 Score:[1.] \n",
      "Episode:10 Score:[3.] \n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = ppo_1000_model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{} '.format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, Evaluate, and Test for 20000 time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to Training\\Logs\\PPO_13\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 662      |\n",
      "|    ep_rew_mean     | 128      |\n",
      "| time/              |          |\n",
      "|    fps             | 112      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 683          |\n",
      "|    ep_rew_mean          | 141          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 82           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 197          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0122767165 |\n",
      "|    clip_fraction        | 0.11         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.78        |\n",
      "|    explained_variance   | 0.0109       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0211       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0175      |\n",
      "|    value_loss           | 0.248        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 670         |\n",
      "|    ep_rew_mean          | 150         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 323         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019590367 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.77       |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00674     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    value_loss           | 0.258       |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Revert to 4 environments\n",
    "env = make_atari_env('SpaceInvaders-v0', n_envs=4, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Train\n",
    "ppo_20000_model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "ppo_20000_model.learn(total_timesteps=20000)\n",
    "ppo_path = os.path.join('Training','Saved Models','PPO_SpaceInvaders_Model')\n",
    "ppo_20000_model.save(ppo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate (Revert back to 1 environment)\n",
    "env = make_atari_env('SpaceInvaders-v0', n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "evaluate_policy(ppo_20000_model, env, n_eval_episodes=10, render=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[4.] \n",
      "Episode:2 Score:[5.] \n",
      "Episode:3 Score:[6.] \n",
      "Episode:4 Score:[5.] \n",
      "Episode:5 Score:[0.] \n",
      "Episode:6 Score:[13.] \n",
      "Episode:7 Score:[1.] \n",
      "Episode:8 Score:[5.] \n",
      "Episode:9 Score:[2.] \n",
      "Episode:10 Score:[5.] \n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = ppo_20000_model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{} '.format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "- Both algorithms appeared to be inconsistent in the testing stage, which indicates that more testing steps are probably required to achieve more consistent results\n",
    "- In terms of accuracy, PPO did achieve the best result, but as discussed above, the results it outputted were inconsistent, and sometimes worse than A2C\n",
    "- A2C actually achieved a higher mean episode length, as well as a higher mean reward per episode, but also produced dramatically higher policy losses and value losses\n",
    "- Overall, from the above results, it could be argued that A2C has higher potential, but PPO is the safer option regarding better performance"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
